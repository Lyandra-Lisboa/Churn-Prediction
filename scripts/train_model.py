"""
train_model.py - Ligga Edition
----------------
Script respons√°vel por treinar o modelo de churn utilizando o algoritmo K-Modes.
Adaptado para trabalhar com as tabelas reais da Ligga.

Inclui:
- Carregamento de dados das tabelas da Ligga
- Pr√©-processamento espec√≠fico da Ligga
- Transforma√ß√£o categ√≥rica otimizada
- Treinamento com K-Modes
- Avalia√ß√£o completa do modelo
- Salvamento do modelo e artefatos
- Registro de m√©tricas detalhadas
"""

import os
import sys
import logging
import joblib
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import argparse
from kmodes.kmodes import KModes

# Importar m√≥dulos espec√≠ficos da Ligga
from extractors import LiggaDataExtractor, validate_data_structure
from preprocessing import preprocess_ligga_data
from transformers import encode_categoricals_advanced, get_categorical_summary
from evaluation import LiggaModelEvaluator

# Configura√ß√£o de logs
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    ]
)

logger = logging.getLogger(__name__)

class LiggaModelTrainer:
    """
    Classe principal para treinamento do modelo de churn da Ligga
    """
    
    def __init__(self, n_clusters: int = 10, encoding_type: str = 'ordinal'):
        """
        Inicializa o trainer
        
        Args:
            n_clusters: N√∫mero de clusters (padr√£o: 10 como nas personas da Ligga)
            encoding_type: Tipo de encoding categ√≥rico ('label', 'ordinal', 'codes')
        """
        self.n_clusters = n_clusters
        self.encoding_type = encoding_type
        
        # Componentes do pipeline
        self.extractor = LiggaDataExtractor()
        self.transformer = None
        self.model = None
        self.evaluator = LiggaModelEvaluator()
        
        # Dados do pipeline
        self.raw_data = None
        self.processed_data = None
        self.encoded_data = None
        self.features_for_model = None
        
        # Resultados
        self.training_results = {}
        
        # Diret√≥rios
        self.artifacts_dir = Path("artifacts")
        self.models_dir = self.artifacts_dir / "models"
        self.data_dir = self.artifacts_dir / "data"
        self.reports_dir = self.artifacts_dir / "reports"
        
        # Criar diret√≥rios
        for dir_path in [self.artifacts_dir, self.models_dir, self.data_dir, self.reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def validate_setup(self) -> bool:
        """
        Valida se o ambiente est√° configurado corretamente
        """
        logger.info("üîç Validando configura√ß√£o do ambiente...")
        
        try:
            # Validar estrutura do banco
            validation = self.extractor.validate_setup()
            
            missing_tables = validation.get('missing_tables', [])
            if missing_tables:
                logger.error(f"‚ùå Tabelas n√£o encontradas: {missing_tables}")
                logger.error("üí° Verifique a configura√ß√£o em config_ligga.py")
                return False
            
            # Verificar se h√° dados suficientes
            row_counts = validation.get('row_counts', {})
            total_records = sum(row_counts.values())
            
            if total_records < 1000:
                logger.warning(f"‚ö†Ô∏è  Poucos dados encontrados: {total_records} registros")
                logger.warning("üìä Recomendamos pelo menos 1000 registros para treinamento")
            
            logger.info("‚úÖ Valida√ß√£o conclu√≠da com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            return False
    
    def load_data(self, limit: int = None) -> pd.DataFrame:
        """
        Carrega dados das tabelas da Ligga
        
        Args:
            limit: Limite de registros (para testes)
            
        Returns:
            DataFrame com dados agregados
        """
        logger.info("üìä Carregando dados das tabelas da Ligga...")
        
        try:
            self.raw_data = self.extractor.extract_aggregated_data(limit=limit)
            
            if self.raw_data.empty:
                raise ValueError("Nenhum dado foi extra√≠do das tabelas")
            
            logger.info(f"‚úÖ Dados carregados: {len(self.raw_data):,} registros, {len(self.raw_data.columns)} colunas")
            
            # Salvar dados brutos
            raw_data_path = self.data_dir / "raw_data.csv"
            self.raw_data.to_csv(raw_data_path, index=False)
            logger.info(f"üíæ Dados brutos salvos em: {raw_data_path}")
            
            return self.raw_data
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar dados: {e}")
            raise
    
    def preprocess_data(self) -> pd.DataFrame:
        """
        Pr√©-processa os dados carregados
        """
        logger.info("üîß Pr√©-processando dados...")
        
        if self.raw_data is None:
            raise ValueError("Dados n√£o foram carregados. Execute load_data() primeiro.")
        
        try:
            self.processed_data = preprocess_ligga_data(self.raw_data)
            
            logger.info(f"‚úÖ Pr√©-processamento conclu√≠do: {len(self.processed_data):,} registros")
            
            # Salvar dados processados
            processed_data_path = self.data_dir / "processed_data.csv"
            self.processed_data.to_csv(processed_data_path, index=False)
            logger.info(f"üíæ Dados processados salvos em: {processed_data_path}")
            
            # Gerar resumo categ√≥rico
            cat_summary = get_categorical_summary(self.processed_data)
            summary_path = self.reports_dir / "categorical_summary.csv"
            cat_summary.to_csv(summary_path, index=False)
            logger.info(f"üìã Resumo categ√≥rico salvo em: {summary_path}")
            
            return self.processed_data
            
        except Exception as e:
            logger.error(f"‚ùå Erro no pr√©-processamento: {e}")
            raise
    
    def transform_data(self) -> pd.DataFrame:
        """
        Aplica transforma√ß√µes categ√≥ricas
        """
        logger.info("üîÑ Aplicando transforma√ß√µes categ√≥ricas...")
        
        if self.processed_data is None:
            raise ValueError("Dados n√£o foram pr√©-processados. Execute preprocess_data() primeiro.")
        
        try:
            # Aplicar encoding categ√≥rico avan√ßado
            self.encoded_data, self.transformer = encode_categoricals_advanced(
                self.processed_data,
                encoding_type=self.encoding_type,
                save_mappings=True
            )
            
            logger.info(f"‚úÖ Transforma√ß√£o conclu√≠da: {len(self.encoded_data.columns)} features")
            
            # Preparar features para o modelo K-Modes
            # Selecionar apenas colunas categ√≥ricas/discretas
            categorical_features = [
                'faixa_valor', 'faixa_aging', 'regiao', 'faixa_velocidade',
                'perfil_atendimento', 'is_churner', 'cliente_tipo',
                'faixa_vencimento', 'tipo_produto', 'canal_produto'
            ]
            
            # Filtrar apenas colunas que existem
            available_features = [col for col in categorical_features if col in self.encoded_data.columns]
            
            if not available_features:
                raise ValueError("Nenhuma feature categ√≥rica dispon√≠vel para o modelo")
            
            self.features_for_model = self.encoded_data[available_features].copy()
            
            logger.info(f"üéØ Features selecionadas para modelo: {available_features}")
            
            # Salvar dados transformados
            encoded_data_path = self.data_dir / "encoded_data.csv"
            self.encoded_data.to_csv(encoded_data_path, index=False)
            
            features_path = self.data_dir / "model_features.csv"
            self.features_for_model.to_csv(features_path, index=False)
            
            logger.info(f"üíæ Dados transformados salvos em: {encoded_data_path}")
            
            return self.features_for_model
            
        except Exception as e:
            logger.error(f"‚ùå Erro na transforma√ß√£o: {e}")
            raise
    
    def train_model(self, init: str = "Huang", n_init: int = 5, max_iter: int = 100, random_state: int = 42) -> KModes:
        """
        Treina o modelo K-Modes
        
        Args:
            init: M√©todo de inicializa√ß√£o
            n_init: N√∫mero de inicializa√ß√µes
            max_iter: M√°ximo de itera√ß√µes
            random_state: Seed para reprodutibilidade
            
        Returns:
            Modelo K-Modes treinado
        """
        logger.info(f"ü§ñ Treinando modelo K-Modes com {self.n_clusters} clusters...")
        
        if self.features_for_model is None:
            raise ValueError("Features n√£o foram preparadas. Execute transform_data() primeiro.")
        
        try:
            # Verificar se h√° dados suficientes
            if len(self.features_for_model) < self.n_clusters * 10:
                logger.warning(f"‚ö†Ô∏è  Poucos dados para {self.n_clusters} clusters")
                logger.warning(f"üìä Recomendamos pelo menos {self.n_clusters * 10} registros")
            
            # Criar e treinar modelo
            self.model = KModes(
                n_clusters=self.n_clusters,
                init=init,
                n_init=n_init,
                max_iter=max_iter,
                verbose=1,
                random_state=random_state
            )
            
            # Treinar
            start_time = datetime.now()
            clusters = self.model.fit_predict(self.features_for_model)
            training_time = (datetime.now() - start_time).total_seconds()
            
            # Adicionar clusters aos dados
            self.encoded_data['cluster'] = clusters
            
            # Registrar resultados
            self.training_results = {
                'n_clusters': self.n_clusters,
                'encoding_type': self.encoding_type,
                'training_time_seconds': training_time,
                'features_used': list(self.features_for_model.columns),
                'cluster_distribution': pd.Series(clusters).value_counts().to_dict(),
                'model_params': {
                    'init': init,
                    'n_init': n_init,
                    'max_iter': max_iter,
                    'random_state': random_state
                }
            }
            
            logger.info(f"‚úÖ Modelo treinado em {training_time:.2f} segundos")
            logger.info(f"üìä Distribui√ß√£o dos clusters: {self.training_results['cluster_distribution']}")
            
            return self.model
            
        except Exception as e:
            logger.error(f"‚ùå Erro no treinamento: {e}")
            raise
    
    def evaluate_model(self) -> dict:
        """
        Avalia o modelo treinado
        """
        logger.info("üìà Avaliando modelo treinado...")
        
        if self.model is None:
            raise ValueError("Modelo n√£o foi treinado. Execute train_model() primeiro.")
        
        try:
            # Configurar evaluator
            self.evaluator.model = self.model
            self.evaluator.data = self.features_for_model
            self.evaluator.processed_data = self.encoded_data
            self.evaluator.clusters = self.encoded_data['cluster'].values
            
            # Executar avalia√ß√£o completa
            evaluation_results = self.evaluator.generate_report()
            
            # Adicionar aos resultados de treinamento
            self.training_results['evaluation'] = evaluation_results
            
            logger.info("‚úÖ Avalia√ß√£o conclu√≠da")
            
            return evaluation_results
            
        except Exception as e:
            logger.error(f"‚ùå Erro na avalia√ß√£o: {e}")
            raise
    
    def save_artifacts(self) -> dict:
        """
        Salva modelo e artefatos relacionados
        """
        logger.info("üíæ Salvando artefatos do modelo...")
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Salvar modelo principal
            model_path = self.models_dir / f"kmodes_ligga_{timestamp}.pkl"
            joblib.dump(self.model, model_path)
            
            # Salvar transformer
            transformer_path = self.models_dir / f"transformer_ligga_{timestamp}.pkl"
            joblib.dump(self.transformer, transformer_path)
            
            # Salvar dados finais com clusters
            final_data_path = self.data_dir / f"final_data_with_clusters_{timestamp}.csv"
            self.encoded_data.to_csv(final_data_path, index=False)
            
            # Salvar resultados de treinamento
            results_path = self.reports_dir / f"training_results_{timestamp}.json"
            import json
            with open(results_path, 'w') as f:
                # Converter numpy types para JSON serializ√°vel
                serializable_results = self._make_json_serializable(self.training_results)
                json.dump(serializable_results, f, indent=2)
            
            # Criar metadata do modelo
            metadata = {
                'timestamp': timestamp,
                'model_path': str(model_path),
                'transformer_path': str(transformer_path),
                'data_path': str(final_data_path),
                'results_path': str(results_path),
                'n_clusters': self.n_clusters,
                'encoding_type': self.encoding_type,
                'total_records': len(self.encoded_data),
                'features_count': len(self.features_for_model.columns),
                'features_used': list(self.features_for_model.columns)
            }
            
            metadata_path = self.models_dir / f"model_metadata_{timestamp}.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Criar link simb√≥lico para o modelo mais recente
            latest_model_path = self.models_dir / "latest_kmodes_ligga.pkl"
            latest_transformer_path = self.models_dir / "latest_transformer_ligga.pkl"
            latest_metadata_path = self.models_dir / "latest_metadata.json"
            
            # Remover links antigos se existirem
            for path in [latest_model_path, latest_transformer_path, latest_metadata_path]:
                if path.exists():
                    path.unlink()
            
            # Criar novos links
            latest_model_path.symlink_to(model_path.name)
            latest_transformer_path.symlink_to(transformer_path.name)
            latest_metadata_path.symlink_to(metadata_path.name)
            
            logger.info(f"‚úÖ Artefatos salvos:")
            logger.info(f"   üì¶ Modelo: {model_path}")
            logger.info(f"   üîÑ Transformer: {transformer_path}")
            logger.info(f"   üìä Dados: {final_data_path}")
            logger.info(f"   üìã Resultados: {results_path}")
            logger.info(f"   üìù Metadata: {metadata_path}")
            
            return metadata
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar artefatos: {e}")
            raise
    
    def _make_json_serializable(self, obj):
        """Converte objetos para formato JSON serializ√°vel"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    def run_full_pipeline(self, limit: int = None) -> dict:
        """
        Executa o pipeline completo de treinamento
        
        Args:
            limit: Limite de registros para teste
            
        Returns:
            Metadata do modelo treinado
        """
        logger.info("üöÄ Iniciando pipeline completo de treinamento...")
        
        try:
            # 1. Validar setup
            if not self.validate_setup():
                raise RuntimeError("Falha na valida√ß√£o do ambiente")
            
            # 2. Carregar dados
            self.load_data(limit=limit)
            
            # 3. Pr√©-processar
            self.preprocess_data()
            
            # 4. Transformar
            self.transform_data()
            
            # 5. Treinar modelo
            self.train_model()
            
            # 6. Avaliar modelo
            self.evaluate_model()
            
            # 7. Salvar artefatos
            metadata = self.save_artifacts()
            
            logger.info("üéâ Pipeline completo executado com sucesso!")
            
            # Resumo final
            logger.info("\n" + "="*60)
            logger.info("üìä RESUMO DO TREINAMENTO")
            logger.info("="*60)
            logger.info(f"   ‚Ä¢ Registros processados: {len(self.encoded_data):,}")
            logger.info(f"   ‚Ä¢ Features utilizadas: {len(self.features_for_model.columns)}")
            logger.info(f"   ‚Ä¢ Clusters criados: {self.n_clusters}")
            logger.info(f"   ‚Ä¢ Encoding usado: {self.encoding_type}")
            logger.info(f"   ‚Ä¢ Tempo de treinamento: {self.training_results.get('training_time_seconds', 0):.2f}s")
            
            if 'evaluation' in self.training_results:
                eval_results = self.training_results['evaluation']
                logger.info(f"   ‚Ä¢ Silhouette Score: {eval_results.get('silhouette_score', 'N/A')}")
                logger.info(f"   ‚Ä¢ Consist√™ncia: {eval_results.get('consistency', 'N/A')}%")
            
            logger.info("="*60)
            
            return metadata
            
        except Exception as e:
            logger.error(f"‚ùå Erro no pipeline: {e}")
            raise

def main():
    """Fun√ß√£o principal para execu√ß√£o via linha de comando"""
    parser = argparse.ArgumentParser(description="Treinar modelo K-Modes para dados da Ligga")
    parser.add_argument("--clusters", type=int, default=10, help="N√∫mero de clusters (padr√£o: 10)")
    parser.add_argument("--encoding", type=str, default="ordinal", choices=["label", "ordinal", "codes"], 
                       help="Tipo de encoding categ√≥rico")
    parser.add_argument("--limit", type=int, help="Limite de registros para teste")
    parser.add_argument("--init", type=str, default="Huang", choices=["Huang", "Cao"], 
                       help="M√©todo de inicializa√ß√£o")
    parser.add_argument("--max-iter", type=int, default=100, help="M√°ximo de itera√ß√µes")
    parser.add_argument("--n-init", type=int, default=5, help="N√∫mero de inicializa√ß√µes")
    
    args = parser.parse_args()
    
    # Criar diret√≥rio de logs
    os.makedirs("logs", exist_ok=True)
    
    try:
        # Inicializar trainer
        trainer = LiggaModelTrainer(
            n_clusters=args.clusters,
            encoding_type=args.encoding
        )
        
        # Executar pipeline
        if args.limit:
            logger.info(f"üß™ Modo teste: limitando a {args.limit} registros")
        
        metadata = trainer.run_full_pipeline(limit=args.limit)
        
        print("\nüéâ Treinamento conclu√≠do com sucesso!")
        print(f"üì¶ Modelo salvo em: {metadata['model_path']}")
        print(f"üìä Metadados dispon√≠veis em: artifacts/models/latest_metadata.json")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  Treinamento interrompido pelo usu√°rio")
        return 1
    except Exception as e:
        logger.error(f"üí• Erro fatal: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
